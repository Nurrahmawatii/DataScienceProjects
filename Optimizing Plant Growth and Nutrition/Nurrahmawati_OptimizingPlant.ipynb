{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBrtsMgUOAJQ"
      },
      "source": [
        "# **\"Tangan Hijau, Teknologi Cerdas: Mengaplikasikan Machine Learning untuk Pertumbuhan dan Nutrisi Tanaman yang Lebih Efisien\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcrDfmY2ChvG"
      },
      "source": [
        "> **NURRAHMAWATI**\n",
        "\n",
        "> Talent Fair Challenge: Aria Dataset\n",
        "\n",
        "Linkedin: https://www.linkedin.com/in/nurrahmawatii/<br>\n",
        "Github: http://github.com/nurrahmawatii<br>\n",
        "Gmail: nurrahmawati682@gmail.com<br>\n",
        "Phone: +6289647118538"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Dsx9-dRl6p"
      },
      "source": [
        "## **I. PENDAHULUAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnK5g2kVjpbK"
      },
      "source": [
        "Data ini berisi informasi mengenai variabel yang dapat digunakan untuk memprediksi nutrisi tanaman. Variabel ini termasuk variabel V1-V8, yang mengandung informasi terkait nutrisi tanaman, serta variabel sample_type, yang mengandung informasi tentang jenis sampel dan laboratorium tempat sampel diperoleh.\n",
        "\n",
        "Proyek ini bertujuan untuk meningkatkan efisiensi dalam pertumbuhan dan nutrisi tanaman dengan menerapkan teknologi cerdas. Data yang digunakan meliputi informasi tentang nutrisi tanaman dan jenis sampel yang berbeda. Dalam proyek ini, machine learning akan digunakan untuk membuat model prediksi yang dapat memprediksi nutrisi tanaman berdasarkan variabel yang relevan.\n",
        "\n",
        "Pada tahap analisis, kita akan menganalisis perbedaan pengaruh jenis sampel yang berbeda terhadap kualitas tanaman, sehingga kita dapat menentukan apakah jenis sampel mempengaruhi pertumbuhan dan nutrisi tanaman secara signifikan.\n",
        "\n",
        "Dalam proyek ini, teknologi cerdas (represented by the term \"tangan hijau, teknologi cerdas\") dan machine learning digunakan untuk memperbaiki pertumbuhan dan nutrisi tanaman dengan cara yang lebih efisien. Diharapkan bahwa proyek ini dapat memberikan manfaat bagi para petani untuk meningkatkan produksi pertanian mereka dan juga bagi lingkungan dengan mengurangi penggunaan pupuk yang tidak perlu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oNd_yR8RyMI"
      },
      "source": [
        "## **II. IMPORT PUSTAKA DAN MEMUAT DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install auto-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "G2FjLLc_Qewb",
        "outputId": "fb26b8b6-6300-439f-94bc-94715edc71ca"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'autosklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32me:\\talent_fair\\Nurrahmawati_TalentFair.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/talent_fair/Nurrahmawati_TalentFair.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline \u001b[39mas\u001b[39;00m imbpipe\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/talent_fair/Nurrahmawati_TalentFair.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/talent_fair/Nurrahmawati_TalentFair.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mautosklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregression\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/talent_fair/Nurrahmawati_TalentFair.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_error \u001b[39mas\u001b[39;00m mae\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/talent_fair/Nurrahmawati_TalentFair.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error \u001b[39mas\u001b[39;00m mse\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autosklearn'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Split Dataset and Standarize the Datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.pipeline import Pipeline as imbpipe\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import autosklearn.regression\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWyLobkNOl4G"
      },
      "source": [
        "Tahap awal yang harus saya lakukan adalah memuat data yang diperoleh dari Google Drive ke dalam notebook saya, sehingga saya tidak perlu lagi mengunggah data secara manual ke dalam notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Cp9O2O0Oj3u"
      },
      "outputs": [],
      "source": [
        "# Connecting google colab runtime with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmRaGQ9KPcef"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "data_ori = pd.read_excel('/content/drive/MyDrive/Talent_Fair_HCK/aria_data.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reQ_Y0lIQizW"
      },
      "outputs": [],
      "source": [
        "data_ori.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XPzr4uXSjKz"
      },
      "outputs": [],
      "source": [
        "data_ori.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RvYJqAfNt3Q"
      },
      "outputs": [],
      "source": [
        "data = data_ori.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJE_oTUDa5El"
      },
      "source": [
        "### 2.1 ANALISIS KUALITAS DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5YjF8zzb577"
      },
      "source": [
        "Analisis kualitas data adalah proses untuk memeriksa dan mengevaluasi kualitas data untuk memastikan bahwa data yang digunakan untuk analisis atau pemodelan adalah akurat, konsisten, lengkap, dan relevan. Dalam analisis kualitas data, kita mencari tahu apakah terdapat data yang hilang, data yang tidak konsisten, atau data yang duplikat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqvT9FEburd"
      },
      "outputs": [],
      "source": [
        "# Checking Basic Information\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNI_bHxVb-Hx"
      },
      "source": [
        "Berdasarkan dari informasi di atas, diketahui bahwa data ini terdiri dari 160 entri data dan terdiri dari total 10 kolom. Kolom tersebut memiliki tipe data float64 dan object. Selain itu, terlihat bahwa tidak terdapat nilai yang kosong dari setiap kolom berdasarkan data yang tersedia. Meskipun demikian, sebaiknya kita melakukan pemeriksaan lebih lanjut untuk memastikan ketiadaan nilai yang kosong pada data tersebut. Dengan melakukan pemeriksaan ini, kita dapat memastikan bahwa data yang digunakan untuk analisis lebih akurat dan valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6vhPflcdROf"
      },
      "outputs": [],
      "source": [
        "# Check Percentage Missing Values\n",
        "data.isnull().mean().sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyGRatoxdbh5"
      },
      "source": [
        "Setelah dilakukan pemeriksaan lebih lanjut, dapat disimpulkan bahwa memang tidak terdapat nilai yang kosong pada data ini. Oleh karena itu, tidak diperlukan adanya penanganan terhadap nilai yang kosong tersebut. Hasil ini memberikan kepastian bahwa data yang digunakan untuk analisis selanjutnya dapat dianggap akurat dan valid. Dengan demikian, data ini dapat dilanjutkan ke tahap analisis berikutnya tanpa harus mengkhawatirkan adanya nilai kosong yang dapat mempengaruhi hasil analisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHZdrK1pd7cS"
      },
      "outputs": [],
      "source": [
        "# Checking data duplicated\n",
        "duplicates = data.duplicated().sum()\n",
        "print('Duplicates:', duplicates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qklJmrhOd9ph"
      },
      "source": [
        "Selain melakukan pemeriksaan terhadap nilai yang kosong pada data, kita juga melakukan pemeriksaan terhadap kemungkinan adanya nilai duplikat pada data. Dari hasil pemeriksaan, dapat disimpulkan bahwa tidak terdapat nilai duplikat pada data yang kita gunakan. Hal ini menunjukkan bahwa data tersebut bersifat unik dan berbeda satu sama lain. Dengan memastikan ketiadaan nilai duplikat pada data, kita dapat memastikan bahwa analisis yang dilakukan nantinya tidak terpengaruh oleh nilai duplikat yang tidak seharusnya ada dalam data tersebut. Dengan demikian, data yang digunakan untuk analisis selanjutnya dapat dianggap valid dan dapat diandalkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyzz-lpQihHA"
      },
      "source": [
        "Selanjutnya, kita memeriksa apakah ada data yang tidak konsisten dalam setiap kolom dengan menghitung standar deviasi dari setiap kolom menggunakan fungsi apply() dan std(). Data yang tidak konsisten bisa menjadi indikasi adanya kesalahan dalam pengambilan data atau pengukuran."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYXO5FaKd8vz"
      },
      "outputs": [],
      "source": [
        "# Check for inconsistent data\n",
        "inconsistent_data = data[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']].apply(lambda x: x.std(), axis=0)\n",
        "print('Inconsistent Data:\\n', inconsistent_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RmGX410hrC-"
      },
      "source": [
        "Dari hasil pemeriksaan yang dilakukan, ditemukan adanya inconsisten data pada kolom V1 hingga V8. Hal ini ditunjukkan oleh nilai yang sangat tinggi pada kolom V8, yaitu 335.035933. Nilai ini jauh lebih besar dibandingkan dengan nilai rata-rata kolom lainnya. Ketidaksesuaian ini dapat menyebabkan bias dalam analisis yang dilakukan karena adanya outlier yang mempengaruhi hasil keseluruhan. Oleh karena itu, sebaiknya kita melakukan pemeriksaan lebih lanjut apakah terdapat outlier atau tidak pada data tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa8ebfaB0ji2"
      },
      "source": [
        "Selanjutnya, untuk memastikan validitas analisis antara kedua sampel, perlu dilakukan pemeriksaan terhadap proporsi dari masing-masing sampel. Jika ditemukan bahwa proporsi kedua sampel tidak sama, maka perlu dilakukan penanganan terhadap kondisi imbalance tersebut sebelum dilakukan proses pemodelan machine learning. Hal ini dilakukan agar hasil analisis yang dihasilkan dapat lebih akurat dan dapat diandalkan dalam mengambil keputusan terkait pertumbuhan tanaman."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXSpS68A1AGs"
      },
      "outputs": [],
      "source": [
        "data['sample_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PXOvzA1FAZ"
      },
      "source": [
        "Berdasarkan hasil dari pemeriksaan di atas bahwa proporsi dari masing-masing sampel tidak seimbang, hal ini dapat memengaruhi kualitas model machine learning yang akan dibangun. Untuk menangani ketidakseimbangan tersebut, dapat dilakukan teknik oversampling, yaitu menambahkan jumlah sampel pada sampel yang kurang hingga proporsi keduanya seimbang. Salah satu teknik oversampling yang umum digunakan adalah Synthetic Minority Oversampling Technique (SMOTE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IZbXq391sQR"
      },
      "outputs": [],
      "source": [
        "# Pisahkan fitur dan label\n",
        "X = data.drop('sample_type', axis=1)\n",
        "y = data['sample_type']\n",
        "\n",
        "# Inisialisasi SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "# Resampling data\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "resampled = pd.concat([X_res.reset_index(drop=True), y_res.reset_index(drop=True)], axis=1)\n",
        "resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFdMnJ0L4o3T"
      },
      "outputs": [],
      "source": [
        "resampled['sample_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0q_3Qmz5k3Z"
      },
      "source": [
        "Hasil dari penerapan SMOTE adalah menghasilkan jumlah sampel yang sama antara kedua jenis sampel, yaitu 100 sampel untuk masing-masing jenis sampel. Hal ini dapat meminimalkan kemungkinan terjadinya overfitting pada model machine learning yang dibangun, serta memastikan bahwa model memiliki kemampuan yang seimbang dalam memprediksi target pada kedua jenis sampel. Dalam hal ini, variabel-variabel yang signifikan terhadap target pada kedua jenis sampel dapat dibandingkan dengan lebih akurat, karena tidak dipengaruhi oleh perbedaan jumlah sampel yang signifikan antara keduanya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMzG56BkTVB"
      },
      "source": [
        "### 2.2 IDENTIFIKASI FAKTOR-FAKTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaUu0u_Gpk55"
      },
      "source": [
        "Identifikasi faktor-faktor yang memengaruhi ketersediaan nutrisi pada tanaman adalah proses analisis untuk menemukan hubungan antara variabel input atau faktor-faktor tertentu dengan ketersediaan nutrisi pada tanaman. Kita memeriksa hubungan antara variabel input atau faktor-faktor dengan ketersediaan nutrisi pada tanaman dengan menggunakan regresi linier.\n",
        "\n",
        "Dalam melakukan analisis regresi, terdapat beberapa asumsi yang harus dipenuhi agar hasil analisis regresi dapat diandalkan. Beberapa asumsi tersebut antara lain normalitas data, homoskedastisitas, dan tidak adanya multikolinearitas antara variabel independent. Oleh karena itu, sebaiknya dilakukan juga uji asumsi regresi sebelum melakukan analisis regresi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrZSVyMn8H8D"
      },
      "source": [
        "#### 2.2.1 NORMALITAS DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuPhz4B5_EZY"
      },
      "source": [
        "Untuk memeriksa normalitas data, kita akan menggunakan uji statistik seperti uji Shapiro-Wilk.\n",
        "\n",
        "Shapiro-Wilk test adalah salah satu tes statistik yang dapat digunakan untuk menguji normalitas data. Hasil dari tes ini dapat dijelaskan sebagai berikut:\n",
        "\n",
        "- H0: Data diuji berasal dari distribusi normal.\n",
        "- H1: Data diuji tidak berasal dari distribusi normal.\n",
        "\n",
        "Jika nilai p (p-value) dari Shapiro-Wilk test lebih besar dari tingkat signifikansi yang telah ditentukan, maka hipotesis nol diterima, dan data dianggap berasal dari distribusi normal. Sebaliknya, jika nilai p lebih kecil dari tingkat signifikansi, maka hipotesis alternatif diterima, dan data dianggap tidak berasal dari distribusi normal.\n",
        "\n",
        "Umumnya, jika nilai p kurang dari 0,05 (tingkat signifikansi 5%), maka dapat dianggap bahwa data tidak berdistribusi normal. Namun, hasil dari Shapiro-Wilk test juga harus dilihat bersamaan dengan plot distribusi data, seperti histogram atau plot density, serta plot qq-plot, untuk memastikan apakah data benar-benar berdistribusi normal atau tidak.\n",
        "\n",
        "QQ-plot (Quantile-Quantile plot) adalah grafik yang menunjukkan plot antara kuantil dari distribusi data dengan kuantil dari distribusi normal. Jika data normal, maka plot akan mengikuti garis diagonal. Sementara jika data tidak normal, plot akan memiliki pola yang berbeda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tx66q1b8EaK"
      },
      "outputs": [],
      "source": [
        "# Plot histogram\n",
        "sns.histplot(resampled['target'], kde=True)\n",
        "\n",
        "# Plot normal probability plot\n",
        "stats.probplot(resampled['target'], dist=\"norm\", plot=plt)\n",
        "\n",
        "# Perform Shapiro-Wilk test\n",
        "shapiro_test = stats.shapiro(resampled['target'])\n",
        "print(\"Shapiro-Wilk test result:\", shapiro_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_Tzpb__uN8"
      },
      "source": [
        "Berdasarkan dari Grafik histogram di atas menunjukkan bahwa data memiliki pola yang simetris dan berbentuk kurva lonceng. Sementara itu, pada grafik QQ-plot, plot data mengikuti garis diagonal, menunjukkan bahwa data berdistribusi normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm0giRu5ATUZ"
      },
      "source": [
        "#### 2.2.2 HOMOSKEDASTISITAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RBQrgxLQuaB"
      },
      "source": [
        "Homoskedastisitas adalah asumsi dalam analisis regresi yang menyatakan bahwa variansi residual (selisih antara nilai sebenarnya dan nilai yang diprediksi) dari model regresi harus sama konstan (homogen) di seluruh rentang nilai dari variabel independen. Dalam kondisi homoskedastisitas, sebaran nilai residual seharusnya merata di sepanjang rentang nilai variabel independen, sehingga tidak terlihat pola tertentu dalam plot residual.\n",
        "\n",
        "Untuk memeriksa homoskedastisitas, kita akan menggunakan scatter plot. Jika pola yang dihasilkan menunjukkan pola funnel atau diamond, maka homoskedastisitas tidak terpenuhi. Jika pola yang dihasilkan menyebar merata dengan variansi yang relatif konstan, maka homoskedastisitas terpenuhi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLI9ZbcbAndV"
      },
      "outputs": [],
      "source": [
        "# define dependent and independent variables\n",
        "y = resampled['target']\n",
        "X = resampled[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']]\n",
        "\n",
        "import statsmodels.api as sm_api\n",
        "\n",
        "# fit linear regression model\n",
        "model = sm_api.OLS(y, sm_api.add_constant(X)).fit()\n",
        "\n",
        "# check homoscedasticity\n",
        "y_pred = model.predict(sm_api.add_constant(X))\n",
        "residuals = y - y_pred\n",
        "\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kg9VJDP6A8I"
      },
      "source": [
        "Berdasarkan hasil dari scatter plot di atas, pola yang dihasilkan tidak menunjukkan pola funnel atau diamond dan menyebar merata dengan variansi yang relatif konstan, maka dapat disimpulkan bahwa homoskedastisitas pada data ini terpenuhi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLVA81MgVL0k"
      },
      "source": [
        "#### 2.2.3 TIDAK ADANYA MULTIKOLINEARITAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-wJSZGWHEI"
      },
      "source": [
        "Untuk memeriksa multikolinearitas, kita akan menggunakan uji statistik seperti uji VIF (variance inflation factor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyaEFpDWWRTY"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF for each variable\n",
        "vif = pd.DataFrame()\n",
        "vif[\"variables\"] = ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']\n",
        "vif[\"VIF\"] = [variance_inflation_factor(resampled[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']].values, i) for i in range(8)]\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1An1bbwttPK"
      },
      "source": [
        "Hasil pemeriksaan multikolinearitas menunjukkan nilai Variance Inflation Factor (VIF) yang cukup tinggi untuk beberapa variabel, yaitu v3, v4, v5, dan v7, dengan nilai VIF masing-masing di atas 1000. Nilai VIF yang tinggi menunjukkan adanya korelasi yang tinggi antara variabel tersebut dengan variabel lain dalam model.\n",
        "\n",
        "Korelasi yang tinggi antar variabel dapat menyebabkan masalah dalam analisis regresi, di mana hasil estimasi parameter regresi dapat menjadi tidak stabil atau tidak dapat diandalkan. Selain itu, interpretasi hasil analisis regresi juga dapat menjadi sulit karena adanya variasi yang tinggi dalam koefisien regresi.\n",
        "\n",
        "Untuk mengatasi masalah multikolinearitas, salah satu langkah yang dapat dilakukan yaitu menghapus menghapus variabel yang memiliki nilai VIF yang sangat tinggi.\n",
        "\n",
        "Dalam hal ini, dapat dipertimbangkan untuk mencoba menghapus variabel v4 dari model karena nilai VIF tertinggi kemudian akan dilakukan pemeriksaan kembali terhadap nilai VIF pada variabel lainnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7fJpIw-urS_"
      },
      "outputs": [],
      "source": [
        "# Drop columns that have highest VIF values\n",
        "resampled.drop(['v4'], axis = 1, inplace=True)\n",
        "resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQkdzzfyBtQB"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF for each variable\n",
        "vif = pd.DataFrame()\n",
        "vif[\"variables\"] = ['v1', 'v2', 'v3', 'v5', 'v6', 'v7', 'v8']\n",
        "vif[\"VIF\"] = [variance_inflation_factor(resampled[['v1', 'v2', 'v3', 'v5', 'v6', 'v7', 'v8']].values, i) for i in range(7)]\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6PMz6WCLD2"
      },
      "source": [
        "Setelah dilakukan pemeriksaan ulang dan penghapusan variabel v4, ditemukan bahwa nilai VIF pada v7 sudah tidak terlalu tinggi seperti sebelumnya. Namun, terdapat variabel v3 yang memiliki nilai VIF yang cukup tinggi dibandingkan dengan variabel lainnya. Oleh karena itu, perlu dilakukan seleksi variabel dengan menghapus v3 dan melakukan pengecekan kembali terhadap variabel lainnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImyJXS0FDFdU"
      },
      "outputs": [],
      "source": [
        "# Drop columns that have highest VIF values\n",
        "resampled.drop(['v3'], axis = 1, inplace=True)\n",
        "resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DvRY2rgDLOI"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF for each variable\n",
        "vif = pd.DataFrame()\n",
        "vif[\"variables\"] = ['v1', 'v2', 'v5', 'v6', 'v7', 'v8']\n",
        "vif[\"VIF\"] = [variance_inflation_factor(resampled[['v1', 'v2', 'v5', 'v6', 'v7', 'v8']].values, i) for i in range(6)]\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1IB9s-hDZNP"
      },
      "source": [
        "Dapat dilihat bahwa setelah dilakukan pemeriksaan ulang, variabel v5 masih memiliki nilai VIF yang cukup tinggi. Oleh karena itu, kita dapat mempertimbangkan untuk mencoba menghapus variabel v5 tersebut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5h8gO9VByNl"
      },
      "outputs": [],
      "source": [
        "# Drop columns that have highest VIF values\n",
        "resampled.drop(['v5'], axis = 1, inplace=True)\n",
        "resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4fRoFMQEE78"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF for each variable\n",
        "vif = pd.DataFrame()\n",
        "vif[\"variables\"] = ['v1', 'v2', 'v6', 'v7', 'v8']\n",
        "vif[\"VIF\"] = [variance_inflation_factor(resampled[['v1', 'v2', 'v6', 'v7', 'v8']].values, i) for i in range(5)]\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E6j0t6AGW7a"
      },
      "source": [
        "Berdasarkan hasil pemeriksaan ulang yang dilakukan, terlihat bahwa nilai VIF pada beberapa variabel telah menurun dan menjadi lebih rendah dibandingkan sebelumnya. Setelah dipertimbangkan, nilai VIF pada semua variabel di atas sudah berada pada tingkat yang dapat diterima, maka tidak perlu lagi dilakukan penghapusan variabel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am1h2vAT493V"
      },
      "source": [
        "#### 2.2.4 ANALISIS REGRESI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2753BHym5-ji"
      },
      "source": [
        "\n",
        "Kita menggunakan formula smf.ols() dari paket statsmodels untuk membangun model regresi linier. Dalam formula regresi linier, Target adalah variabel dependent atau target yang ingin kita prediksi, V1, V2, V6, dan V8 adalah variabel independent atau faktor-faktor yang memengaruhi ketersediaan nutrisi pada tanaman, dan Sample_Type adalah variabel kategorikal yang mewakili dua jenis sampel yang diperoleh dari dua laboratorium yang berbeda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNoZgYhnkcMq"
      },
      "outputs": [],
      "source": [
        "# Fit a linear regression model\n",
        "model = smf.ols('target ~ v1 + v2 + v6 + v7 + v8 + sample_type', data=resampled).fit()\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW5qVYuboKbf"
      },
      "source": [
        "Hasil yang diberikan merupakan output dari regresi linear berganda (multiple linear regression) dengan menggunakan Ordinary Least Squares (OLS) sebagai metode estimasi. Model regresi ini mencoba untuk menjelaskan variasi pada variabel target (dependent variable) dengan menggunakan beberapa variabel prediktor (independent variable) yang tercantum dalam tabel.\n",
        "\n",
        "Beberapa informasi yang dapat diambil dari hasil regresi ini antara lain:\n",
        "\n",
        "\n",
        "*   R-squared (R2) atau koefisien determinasi sebesar 0.310. R2 adalah ukuran seberapa baik variabel independen dapat menjelaskan variasi pada variabel dependen. Semakin tinggi nilai R2, semakin baik variabel independen dalam menjelaskan variasi pada variabel dependen. Nilai R2 sebesar 0.310 menunjukkan bahwa variabel independen yang digunakan dalam model hanya dapat menjelaskan sekitar 31.0% variasi pada variabel dependen, sehingga masih ada sekitar 69.0% variasi lain yang belum dijelaskan oleh variabel independen.\n",
        "\n",
        "*   Adj.R-squared (Adjusted R2) sebesar 0.288. Nilai ini adalah R2 yang disesuaikan untuk jumlah variabel independen yang digunakan dalam model. Semakin banyak variabel independen yang digunakan dalam model, semakin tinggi nilai Adj.R2, namun jika variabel independen yang ditambahkan tidak signifikan, nilai Adj.R2 malah dapat turun. Nilai Adj.R2 ini lebih rendah dari R2, yang menunjukkan bahwa penambahan variabel independen dalam model belum memberikan kontribusi signifikan terhadap penjelasan variasi pada variabel dependen.\n",
        "\n",
        "*   F-statistic sebesar 14.44, dengan Prob (F-statistic) atau p-value sebesar 1.38e-13. F-statistic adalah ukuran kebermaknaan model regresi secara keseluruhan. Semakin tinggi nilai F-statistic, semakin baik model dalam menjelaskan variasi pada variabel dependen. Pada hasil di atas, nilai F-statistic cukup tinggi dengan p-value yang sangat rendah, sehingga model regresi dapat dianggap signifikan secara statistik.\n",
        "\n",
        "*   Durbin-Watson (DW) sebesar 2.049. DW adalah ukuran autokorelasi pada residual (sisa) model regresi. Nilai DW berkisar antara 0 dan 4, dengan nilai 2 menunjukkan bahwa tidak ada autokorelasi yang signifikan pada residual model. Pada hasil di atas, nilai DW sebesar 2.049 menunjukkan bahwa tidak ada autokorelasi yang signifikan pada residual model.\n",
        "\n",
        "*   Tabel koefisien menunjukkan nilai estimasi koefisien, standar error (std err), t-statistic, dan p-value untuk setiap variabel independen dalam model. Koefisien mengindikasikan arah dan besar pengaruh variabel independen terhadap variabel dependen. Nilai t-statistic dan p-value digunakan untuk menentukan signifikansi statistik dari koefisien. Jika nilai p-value kurang dari alpha (tingkat signifikansi yang ditentukan sebelumnya), maka koefisien dianggap signifikan secara statistik. Dalam hasil di atas, hanya variabel Intercept yang signifikan secara statistik dengan p-value 0.000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA7uoe2V2e6B"
      },
      "source": [
        "Berdasarkan hasil output tersebut, tidak dapat ditarik kesimpulan bahwa semua variabel independen memiliki pengaruh signifikan terhadap variabel target. Hal ini dapat dilihat dari nilai p-nilai (P>|t|) untuk setiap variabel independen, di mana semua nilai p-nilai melebihi tingkat signifikansi yang umumnya digunakan (biasanya 0,05).\n",
        "\n",
        "Namun, perlu diperhatikan bahwa keberadaan pengaruh tidak signifikan secara statistik (terlihat dari nilai p-nilai) tidak berarti bahwa variabel independen tersebut sama sekali tidak memiliki pengaruh terhadap variabel target.\n",
        "\n",
        "Oleh karena itu, diperlukan analisis yang lebih mendalam untuk menentukan variabel mana yang benar-benar memiliki pengaruh signifikan pada variabel target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBStkHEUFnAM"
      },
      "source": [
        "**Analisis terhadap dua tipe sampel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihiwzrCcFlk9"
      },
      "source": [
        "Pada sesi ini akan dilakukan uji t-test antara dua sampel pada kolom target dari dataset yang tersedia.\n",
        "\n",
        "Hasil dari ttest_ind adalah t-statistic dan p-value. t-statistic merupakan nilai t dari uji hipotesis t-test, sedangkan p-value merupakan nilai probabilitas yang menunjukkan seberapa signifikan perbedaan antara kedua sampel. Semakin kecil p-value, semakin signifikan perbedaan antara kedua sampel.\n",
        "\n",
        "Setelah melakukan uji hipotesis, kita dapat mengevaluasi hasilnya berdasarkan nilai p-value. Jika nilai p-value lebih kecil dari tingkat signifikansi yang ditetapkan (biasanya 0.05), maka kita dapat menolak hipotesis nol dan menyimpulkan bahwa terdapat perbedaan yang signifikan antara kedua sampel. Sebaliknya, jika nilai p-value lebih besar dari tingkat signifikansi, maka kita gagal menolak hipotesis nol dan menyimpulkan bahwa tidak terdapat perbedaan yang signifikan antara kedua sampel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO5UeSWLCtiD"
      },
      "outputs": [],
      "source": [
        "sample1 = resampled[resampled['sample_type'] == 'lab 1']['target']\n",
        "sample2 = resampled[resampled['sample_type'] == 'lab 2']['target']\n",
        "\n",
        "# perform t-test\n",
        "t_stat, p_value = ttest_ind(sample1, sample2)\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v54O1kfACB"
      },
      "source": [
        "Berdasarkan hasil pemeriksaan di atas menunjukkan bahwa t-statistik adalah -0,93 dan p-value adalah 0,35. Dalam kasus ini, p-value lebih besar dari 0,05, sehingga kita tidak dapat menolak hipotesis nol, yaitu tidak terdapat perbedaan signifikan antara kedua sampel. Oleh karena itu, dapat disimpulkan bahwa tidak terdapat perbedaan signifikan antara kedua sampel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRpCHnzHhOV8"
      },
      "source": [
        "Meskipun hasil uji t tidak menunjukkan adanya perbedaan yang signifikan antara kedua sampel tersebut, namun masih dapat menggunakan variabel yang telah dipilih untuk melakukan prediksi menggunakan model machine learning. Karena tidak ada perbedaan signifikan antara sampel yang berbeda, maka kita bisa menganggap bahwa faktor-faktor yang mempengaruhi target pada kedua sampel tersebut relatif sama dan dapat dipertimbangkan secara serupa saat membangun model machine learning. Namun, perlu diingat bahwa tidak ada jaminan bahwa model yang dibangun dengan data tersebut akan memiliki kinerja yang sama baiknya pada kedua sampel. Hal ini karena meskipun faktor-faktor yang mempengaruhi target pada kedua sampel tersebut relatif sama, tetapi nilai dan kisaran dari masing-masing faktor pada kedua sampel tersebut bisa berbeda. Oleh karena itu, pada project ini tetap dipertimbangkan perbedaan karakteristik antara kedua sampel dalam membangun model machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIs_bvhH0_f1"
      },
      "outputs": [],
      "source": [
        "# Split data into two samples\n",
        "lab1 = resampled[resampled['sample_type'] == 'lab 1']\n",
        "lab2 = resampled[resampled['sample_type'] == 'lab 2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmBUW9xIDBH0"
      },
      "outputs": [],
      "source": [
        "# Drop columns sample_type\n",
        "lab1.drop(['sample_type'], axis = 1, inplace=True)\n",
        "lab2.drop(['sample_type'], axis = 1, inplace=True)\n",
        "lab2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff76OsxNSEX1"
      },
      "source": [
        "## **III. EXPLORATORY DATA ANALYSIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkD7IjnLIh5U"
      },
      "source": [
        "## **IV. DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP4QJimYRHU3"
      },
      "source": [
        "### 4.1 SPLITTING THE DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmt6G5D-iquK"
      },
      "source": [
        "#### 4.1.1 SAMPLE_TYPE:LAB 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0B7MBikwGa"
      },
      "outputs": [],
      "source": [
        "# separating inference data\n",
        "inference_lab1 = lab1.sample(10, random_state=32)\n",
        "# reseting index\n",
        "inference_lab1.reset_index(drop=True, inplace=True)\n",
        "inference_lab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GDQ-IFak6a3"
      },
      "outputs": [],
      "source": [
        "# separating inferential data from dataframe\n",
        "lab1 = lab1.drop(inference_lab1.index)\n",
        "# reseting index\n",
        "lab1.reset_index(drop=True, inplace=True)\n",
        "lab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBIrNxkhi3ji"
      },
      "outputs": [],
      "source": [
        "# inference_lab1 = lab1.sample(10, random_state=15)\n",
        "# lab1 = lab1.drop(inference_lab1.index)\n",
        "# lab1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Myq2LOYbhZ"
      },
      "outputs": [],
      "source": [
        "# define feature and target\n",
        "X_lab1 = lab1.drop('target', axis=1)\n",
        "y_lab1 = lab1.target\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_lab1, y_lab1, test_size=0.2, random_state=42)\n",
        "\n",
        "for i in [X_train1, X_test1, y_train1, y_test1]:\n",
        "    print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGICkCiUYjXy"
      },
      "outputs": [],
      "source": [
        "# for EDA model creation, use dataframe stored in data_eda_lab1\n",
        "data_eda_lab1 = pd.concat([X_train1.reset_index(drop=True), y_train1.reset_index(drop=True)], axis=1)\n",
        "data_eda_lab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i29LsjAm-YP"
      },
      "outputs": [],
      "source": [
        "# Numerical Overview\n",
        "data_eda_lab1.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxwHWcc1pOzt"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=3,nrows=2, figsize=[15,10])\n",
        "ax = ax.flatten()\n",
        "\n",
        "for idx, col in enumerate(data_eda_lab1):\n",
        "    sns.histplot(data_eda_lab1[col], ax=ax[idx])\n",
        "    ax[idx].set_title(f'{[idx]} skew: {data_eda_lab1[col].skew()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz7dhE3BuEbd"
      },
      "outputs": [],
      "source": [
        "def outlier_analysis(data_eda_lab1,col):\n",
        "  skewness = data_eda_lab1[col].skew()\n",
        "  if skewness>=-0.5 or skewness<=0.5:\n",
        "    upper = data_eda_lab1[col].mean() + 3*data_eda_lab1[col].std()\n",
        "    lower = data_eda_lab1[col].mean() - 3*data_eda_lab1[col].std()\n",
        "  else:\n",
        "    Q1 = data_eda_lab1[col].quantile(0.25)\n",
        "    Q3 = data_eda_lab1[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper = Q1 + (3*IQR)\n",
        "    lower = Q3 - (3*IQR)\n",
        "  \n",
        "  no_outliers = data_eda_lab1[(data_eda_lab1[col]>=lower) & (data_eda_lab1[col]<=upper)]\n",
        "  outliers = data_eda_lab1[(data_eda_lab1[col]<lower) | (data_eda_lab1[col]>upper)]\n",
        "  print('percentage outlier from',i,':',outliers.shape[0]/data_eda_lab1.shape[0] * 100, '%')\n",
        "  return outliers,no_outliers, upper, lower\n",
        "\n",
        "for i in list(data_eda_lab1.columns):\n",
        "  outlier_analysis(data_eda_lab1,i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBz332y9u28V"
      },
      "outputs": [],
      "source": [
        "wins = Winsorizer(capping_method='iqr', tail='both', fold=1.5, missing_values='ignore', variables=['v1', 'v6', 'v8'])\n",
        "\n",
        "lab1_cleaned = wins.fit_transform(data_eda_lab1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2t-45tcv4RX"
      },
      "outputs": [],
      "source": [
        "# Compare before and after outlier handling\n",
        "print('before handling: \\n', data_eda_lab1.describe())\n",
        "\n",
        "print('after handling: \\n', lab1_cleaned.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLkA2-l-u-_n"
      },
      "source": [
        "**SELECTION FEATURES NUMERIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaSaojg7uApk"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(figsize=[20,15])\n",
        "\n",
        "corr = lab1_cleaned.corr()\n",
        "\n",
        "ax = sns.heatmap(corr,annot=True, vmin=0, vmax=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtyowIcLu7vv"
      },
      "source": [
        "**SCALING FEATURES NUMERIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhCiFJOivVtO"
      },
      "outputs": [],
      "source": [
        "norm = 0\n",
        "nonorm = 0\n",
        "for col in lab1_cleaned[['v1', 'v2', 'v6', 'v7', 'v8']]:\n",
        "  if lab1_cleaned[col].skew() >=-0.5 and lab1_cleaned[col].skew() <0.5:\n",
        "    norm += 1\n",
        "  else:\n",
        "    nonorm +=1\n",
        "\n",
        "if norm > nonorm:\n",
        "  scaler = StandardScaler()\n",
        "else:\n",
        "  scaler = MinMaxScaler()\n",
        "scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zHtqTxUvjXp"
      },
      "outputs": [],
      "source": [
        "# numerical scaling\n",
        "num_col_scalling = ['v1', 'v2', 'v6', 'v7', 'v8']\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "num_scaled = pd.DataFrame(scaler.fit_transform(lab1_cleaned[num_col_scalling]))\n",
        "num_scaled.columns = num_col_scalling\n",
        "\n",
        "lab1_cleaned.drop(num_col_scalling, axis=1, inplace=True)\n",
        "lab1_cleaned = pd.concat([lab1_cleaned.reset_index(drop=True), num_scaled], axis=1)\n",
        "\n",
        "lab1_cleaned_num = lab1_cleaned[['v1', 'v2', 'v6', 'v7', 'v8', 'target']]\n",
        "lab1_cleaned_num.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxAJ-6oa0OfQ"
      },
      "source": [
        "#### 4.1.2 SAMPLE_TYPE:LAB 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lplAC-AKlNws"
      },
      "outputs": [],
      "source": [
        "# separating inference data\n",
        "inference_lab2 = lab2.sample(10, random_state=32)\n",
        "# reseting index\n",
        "inference_lab2.reset_index(drop=True, inplace=True)\n",
        "inference_lab2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3M3ps05lPlo"
      },
      "outputs": [],
      "source": [
        "# separating inferential data from dataframe\n",
        "lab2 = lab2.drop(inference_lab2.index)\n",
        "# reseting index\n",
        "lab2.reset_index(drop=True, inplace=True)\n",
        "lab2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YipGcc9e0ScI"
      },
      "outputs": [],
      "source": [
        "# inference_lab2 = lab2.sample(10, random_state=15)\n",
        "# lab2 = lab2.drop(inference_lab2.index)\n",
        "# lab2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acJCrPxm0cJQ"
      },
      "outputs": [],
      "source": [
        "# define feature and target\n",
        "X_lab2 = lab2.drop('target', axis=1)\n",
        "y_lab2 = lab2.target\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_lab2, y_lab2, test_size=0.2, random_state=42)\n",
        "\n",
        "for i in [X_train2, X_test2, y_train2, y_test2]:\n",
        "    print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A07cBpu0eSo"
      },
      "outputs": [],
      "source": [
        "# for EDA model creation, use dataframe stored in data_eda_lab2\n",
        "data_eda_lab2 = pd.concat([X_train2.reset_index(drop=True), y_train2.reset_index(drop=True)], axis=1)\n",
        "data_eda_lab2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d956hUjk00ft"
      },
      "outputs": [],
      "source": [
        "# Numerical Overview\n",
        "data_eda_lab2.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5BGQfWc03X4"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=3,nrows=2, figsize=[15,10])\n",
        "ax = ax.flatten()\n",
        "\n",
        "for idx, col in enumerate(data_eda_lab2):\n",
        "    sns.histplot(data_eda_lab2[col], ax=ax[idx])\n",
        "    ax[idx].set_title(f'{[idx]} skew: {data_eda_lab2[col].skew()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB-SH3921FPj"
      },
      "outputs": [],
      "source": [
        "def outlier_analysis(data_eda_lab2,col):\n",
        "  skewness = data_eda_lab2[col].skew()\n",
        "  if skewness>=-0.5 or skewness<=0.5:\n",
        "    upper = data_eda_lab2[col].mean() + 3*data_eda_lab2[col].std()\n",
        "    lower = data_eda_lab2[col].mean() - 3*data_eda_lab2[col].std()\n",
        "  else:\n",
        "    Q1 = data_eda_lab2[col].quantile(0.25)\n",
        "    Q3 = data_eda_lab2[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper = Q1 + (1.5*IQR)\n",
        "    lower = Q3 - (1.5*IQR)\n",
        "  \n",
        "  no_outliers = data_eda_lab2[(data_eda_lab2[col]>=lower) & (data_eda_lab2[col]<=upper)]\n",
        "  outliers = data_eda_lab2[(data_eda_lab2[col]<lower) | (data_eda_lab2[col]>upper)]\n",
        "  print('percentage outlier from',i,':',outliers.shape[0]/data_eda_lab2.shape[0] * 100, '%')\n",
        "  return outliers,no_outliers, upper, lower\n",
        "\n",
        "for i in list(data_eda_lab2.columns):\n",
        "  outlier_analysis(data_eda_lab2,i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwLFaJHY1aTT"
      },
      "source": [
        "**SELECTION FEATURES NUMERIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52cJaPsU1dDw"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(figsize=[20,15])\n",
        "\n",
        "corr = data_eda_lab2.corr()\n",
        "\n",
        "ax = sns.heatmap(corr,annot=True, vmin=0, vmax=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdyF0cx52pSW"
      },
      "source": [
        "**SCALING FEATURES NUMERIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlJL-DpFFb6F"
      },
      "outputs": [],
      "source": [
        "norm = 0\n",
        "nonorm = 0\n",
        "for col in data_eda_lab2[['v1', 'v2', 'v6', 'v7', 'v8']]:\n",
        "  if data_eda_lab2[col].skew() >=-0.5 and data_eda_lab2[col].skew() <0.5:\n",
        "    norm += 1\n",
        "  else:\n",
        "    nonorm +=1\n",
        "\n",
        "if norm > nonorm:\n",
        "  scaler = StandardScaler()\n",
        "else:\n",
        "  scaler = MinMaxScaler()\n",
        "scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUO1UiL_2wD7"
      },
      "outputs": [],
      "source": [
        "# numerical scaling\n",
        "num_col_scalling = ['v1', 'v2', 'v6', 'v7', 'v8']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_scaled = pd.DataFrame(scaler.fit_transform(data_eda_lab2[num_col_scalling]))\n",
        "num_scaled.columns = num_col_scalling\n",
        "\n",
        "data_eda_lab2.drop(num_col_scalling, axis=1, inplace=True)\n",
        "data_eda_lab2 = pd.concat([data_eda_lab2.reset_index(drop=True), num_scaled], axis=1)\n",
        "\n",
        "data_eda_lab2_num = data_eda_lab2[['v1', 'v2', 'v6', 'v7', 'v8', 'target']]\n",
        "data_eda_lab2_num.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9lJBamU4Nuq"
      },
      "source": [
        "### 4.2 PREPROCESS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVrgxuCYcT7"
      },
      "source": [
        "#### 4.2.1 TESTING:LAB 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1yqzC_E5FUD"
      },
      "outputs": [],
      "source": [
        "testing_lab1 = pd.concat([X_test1.reset_index(drop=True), y_test1.reset_index(drop=True)], axis=1)\n",
        "testing_lab1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJxTU6nD6oiW"
      },
      "outputs": [],
      "source": [
        "# capping outlier\n",
        "testing_lab1 = wins.transform(testing_lab1)\n",
        "testing_lab1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOIThkbSTPfX"
      },
      "outputs": [],
      "source": [
        "# numerical scalling\n",
        "num_col = ['v1', 'v2', 'v6', 'v7', 'v8']\n",
        "\n",
        "testing_lab1[num_col] = scaler.transform(testing_lab1[num_col])\n",
        "testing_lab1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBB4nnOuYJ2M"
      },
      "outputs": [],
      "source": [
        "X_train1 = lab1_cleaned_num.drop('target',axis=1).copy()\n",
        "y_train1 = lab1_cleaned_num['target']\n",
        "\n",
        "X_test1 = testing_lab1.drop('target',axis=1).copy()\n",
        "y_test1 = testing_lab1['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJwaknFQZBYg"
      },
      "outputs": [],
      "source": [
        "X_train1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUSQ9Sx_ZFZj"
      },
      "outputs": [],
      "source": [
        "X_test1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXkMhtBiYjC9"
      },
      "source": [
        "#### 4.2.2 TESTING:LAB 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3VVCCq4ZbGp"
      },
      "outputs": [],
      "source": [
        "testing_lab2 = pd.concat([X_test2.reset_index(drop=True), y_test2.reset_index(drop=True)], axis=1)\n",
        "testing_lab2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOIK4LdBZorx"
      },
      "outputs": [],
      "source": [
        "# numerical scalling\n",
        "num_col = ['v1', 'v2', 'v6', 'v7', 'v8']\n",
        "\n",
        "testing_lab2[num_col] = scaler.transform(testing_lab2[num_col])\n",
        "testing_lab2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM3DGphsZYvU"
      },
      "outputs": [],
      "source": [
        "X_train2 = data_eda_lab2_num.drop('target',axis=1).copy()\n",
        "y_train2 = data_eda_lab2_num['target']\n",
        "\n",
        "X_test2 = testing_lab2.drop('target',axis=1).copy()\n",
        "y_test2 = testing_lab2['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaEFDQrWZ61p"
      },
      "outputs": [],
      "source": [
        "X_train2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAy0Rg4lZ97m"
      },
      "outputs": [],
      "source": [
        "X_test2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZkSh85el5Z7"
      },
      "source": [
        "## V. MODEL TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IvSbiLxfTaL"
      },
      "source": [
        "#### 5.1.1 SAMPLE_TYPE:LAB 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQAGHPell12K"
      },
      "outputs": [],
      "source": [
        "auto_regressor1 = autosklearn.regression.AutoSklearnRegressor(\n",
        "    time_left_for_this_task=300,\n",
        "    per_run_time_limit=60,\n",
        ")\n",
        "auto_regressor1.fit(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xpkUFSXd5JL"
      },
      "outputs": [],
      "source": [
        "print(auto_regressor1.leaderboard())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpXbhd4fXsE"
      },
      "source": [
        "#### 5.1.2 SAMPLE_TYPE:LAB 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXnRce-1fE-v"
      },
      "outputs": [],
      "source": [
        "auto_regressor2 = autosklearn.regression.AutoSklearnRegressor(\n",
        "    time_left_for_this_task=300,\n",
        "    per_run_time_limit=60,\n",
        ")\n",
        "auto_regressor2.fit(X_train2, y_train2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKaIR-1efepm"
      },
      "outputs": [],
      "source": [
        "print(auto_regressor2.leaderboard())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15F5MROgVXL"
      },
      "source": [
        "## VI. MODEL EVALUATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkg2fAlbgcgu"
      },
      "source": [
        "#### 6.1.1 SAMPLE_TYPE:LAB 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHERURsUgmYS"
      },
      "outputs": [],
      "source": [
        "train_predict1 = auto_regressor1.predict(X_train1)\n",
        "test_predict1 = auto_regressor1.predict(X_test1)\n",
        "\n",
        "print('------- TRAIN EVALUATION -------')\n",
        "print('MAE :', mae(y_train1, train_predict1))\n",
        "print('RMSE:', np.sqrt(mse(y_train1, train_predict1)))\n",
        "print(' ')\n",
        "print('------- TEST EVALUATION -------')\n",
        "print('MAE :', mae(y_test1, test_predict1))\n",
        "print('RMSE:', np.sqrt(mse(y_test1, test_predict1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dqnCHaSgflw"
      },
      "source": [
        "#### 6.1.2 SAMPLE_TYPE:LAB 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uhjoyATgbZC"
      },
      "outputs": [],
      "source": [
        "train_predict2 = auto_regressor2.predict(X_train2)\n",
        "test_predict2 = auto_regressor2.predict(X_test2)\n",
        "\n",
        "print('------- TRAIN EVALUATION -------')\n",
        "print('MAE :', mae(y_train2, train_predict2))\n",
        "print('RMSE:', np.sqrt(mse(y_train2, train_predict2)))\n",
        "print(' ')\n",
        "print('------- TEST EVALUATION -------')\n",
        "print('MAE :', mae(y_test2, test_predict2))\n",
        "print('RMSE:', np.sqrt(mse(y_test2, test_predict2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iXGtOsojqW4"
      },
      "source": [
        "## VII. MODEL INFERENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4co_stMj04q"
      },
      "source": [
        "#### 7.1.1 SAMPLE_TYPE:LAB 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwEVBvk1j0hQ"
      },
      "outputs": [],
      "source": [
        "inference_lab1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ntVAx2Kj2pP"
      },
      "source": [
        "#### 7.1.2 SAMPLE_TYPE:LAB 2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
